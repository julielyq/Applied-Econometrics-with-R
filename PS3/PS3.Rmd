---
title: "Problem Set 3"
author: "Written By Yunqiu (Julie) Li, in Autumn 2018"
output:
  html_document:
    df_print: paged
  pdf_document: default
---


```{r setup, include=FALSE, warning = FALSE, echo = FALSE}
knitr::opts_knit$set(root.dir = getwd())
setwd("~/Desktop/Fall 2018/applied econometrics with R/Yunqiu(Julie) Li PS3")
remove(list = ls())
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
  library(tidyverse)
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
  library(knitr)
if(!require(sandwich)) install.packages("sandwich", repos = "http://cran.us.r-project.org")
  library(sandwich)
if(!require(lmtest)) install.packages("lmtest", repos = "http://cran.us.r-project.org")
  library(lmtest)
# read in data: birthweight_smoking.csv
birthweight_smoking <- read.csv("birthweight_smoking.csv")
cps12 <- read.csv("cps12.csv")
```

## Question 7.1
### a. 
```{r}
# regression 1: Birthweight on Smoker
reg_1 <-lm(birthweight~smoker, data = birthweight_smoking)
coeftest(reg_1, vcov = vcovHC(reg_1, type = "HC1"))
# regression 2: Birthweight on Smoker, Alcohol, and Nprevist
reg_2 <-lm(birthweight~smoker+alcohol+nprevist, data = birthweight_smoking)
coeftest(reg_2, vcov = vcovHC(reg_2, type = "HC1"))
# regression 3: Birthweight on Smoker, Alcohol, Nprevist, and Unmarried
reg_3 <-lm(birthweight~smoker+alcohol+nprevist+unmarried, data = birthweight_smoking)
coeftest(reg_3, vcov = vcovHC(reg_3, type = "HC1"))
```
The estimated effect of smoking on birth weight in the first regression is  -253.228. The estimated effect of smoking on birth weight in the second regression is -217.580. The estimated effect of smoking on birth weight in the third regression is -175.377. 

### b.
```{r}
# 95% confidenct interval = [estimated coefficient +/- (1.96*sdr. error for estimated coefficient)]
# CI for regression 1
c(-253.228 - 1.96*26.810, -253.228 + 1.96*26.810)
# CI for regression 2
c(-217.5801 - 1.96*26.1076, -217.5801 + 1.96*26.1076)
# CI for regression 3
c(-175.3769 - 1.96*26.8268, -175.3769 + 1.96*26.8268)
```
The 95% confidence interval for the effect of smoking on birth weight in regression 1 is [-305.7756, -200.6804].
The 95% confidence interval for the effect of smoking on birth weight in regression 2 is [-268.7510, -166.4092].
The 95% confidence interval for the effect of smoking on birth weight in regression 3 is [-227.9574, -122.7964].

### c.
```{r}
# Compare coffecient and adjusted R-squared for regression 1&2
summary(reg_1)
summary(reg_2)
```
Regression 1 suffers from omitted variable bias. Alcohol and nprevist are very like to be correlated with smoker and they are also potential determinants of birthweight. We can also tell that from the two regressions above. As alcohol and nprevist are added to regression 2, the coefficent of smoker increases from -253.23 to -217.580, and the adjusted R-squared increases from 0.02828 to 0.07192, which indicates that regression 2 is a better estimator for birthweight. The adjusted R-squared increases because two more regressors(alcohol and nprevist) are added to the regression, which adjusts some of the omitted variable bias.

### d.
```{r}
# Compare coffecient and adjusted R-squared for regression 2&3
summary(reg_2)
summary(reg_3)
```
Regression 2 also suffers from omitted variable bias. Unmarried are very like to be correlated with smoker and it is also potential determinant of birthweight. We can also tell that from the two regressions above. As unmarried is added to regression 3, the coefficent of smoker increases from -217.580 to -175.377, and the adjusted R-squared increases from 0.07192 to 0.08739, which indicates that regression 3 is the best estimator for birthweight among three regressions. The adjusted R-squared increases because regressor unmarried is added to the regression, which adjusts some of the omitted variable bias.

### e.
```{r}
# i.
confint(reg_3,"unmarried",level=0.95)
```
The 95% confidence internval for the coefficient on Unmarried in regression 3 is [238.1276, -136.1389].

```{r}
# ii.
# generate coefficient and p-value for unmarred in regression 3
coeftest(reg_3, vcov = vcovHC(reg_3, type = "HC1"))
```
The coefficient for unmarried in regression 3 is statistically significant, because the coefficient -187.1332 is within the 95% confidence internval for the coefficient. Also, the absolute value for t-stat for unmarried is 6.7613, which is greater than the critical value of 2.58, so the coefficient for unmarried in regression 3 is also statistically significant at 1% significant level.

iii. The magnitude of the coefficient is large. From the above regression result, we can tell that unmarried actually have the biggest effect on birthweight compared with the other varaibles do, because the abosulte value for its coefficient is the greatest among the absoluate value for all the variables coefficient.

iv. I disagree that the large coefficient suggests that public policies that encourage marriage will lead, on average, to healthier babies. Because unmarried is a control variable that controls for factors like nutritious intakes. Marriage will generately improve the financial situation of the family as the couples both have income sources, which wil enable mothers in those families to have greater access to nutritious intakes.Therefore, in this situation, the control variable unmarriage remains correlated with the error term, so the coefficient on unmarried is subjected to omitted variable bias and does not have a casual interpretation. 

### f.
```{r}
# Initial Regression
reg_4 <-lm(birthweight~smoker+alcohol+nprevist+unmarried+drinks+tripre1+tripre2+tripre3, data = birthweight_smoking)
coeftest(reg_4, vcov = vcovHC(reg_4, type = "HC1"))
# Updated Regression
reg_5 <-lm(birthweight~smoker+alcohol+nprevist+unmarried+tripre2+tripre3, data = birthweight_smoking)
coeftest(reg_5, vcov = vcovHC(reg_5, type = "HC1"))
# 95% confidence interval
confint(reg_5,"smoker",level=0.95)
```
I think variables tripre2 and tripre3 should also be included in the regression. Initially, I think tripre1, tripre2, tripre3 and drinks should all be included in the regression, because the time for prenatal care visit and the numbers of drink would affect birthweight. However, the coefficient for these two variables are not statistically significant. The coefficients for tripre2 and tripre3 are statistically significant. The 95% confidence interval for the effect of smoking on birthweight is [-232.3486, -126.1843].

## Question 8.2
### a.
```{r}
# regression of average hourly earnings (AHE) on age (Age), gender (Female), and education (Bachelor)
reg_ahe_1 <- lm(ahe~age+female+bachelor, data = cps12)
coeftest(reg_ahe_1, vcov = vcovHC(reg_ahe_1, type = "HC1"))
```
Based on the above regression output，if Age increases from 25 to 26, earnings are expected to increase 0.51. Similarly, if Age increases from 33 to 34, earnings are also expected to increase 0.51. The regression model above is a linear model. Therefore, the coefficient on age indicates the effect of Age on AHE when age change by one unit, which is 0.51.

### b.
```{r}
# add column of log(ahe) in dataset
cps12 <- cps12 %>% mutate(logearning = log(ahe))
# log-linear regression. Run a regression of the logarithm of average hourly earnings, ln(AHE), on Age, Female, and Bachelor.
reg_ahe_2 <- lm(logearning~age+female+bachelor, data = cps12)
coeftest(reg_ahe_2, vcov = vcovHC(reg_ahe_2, type = "HC1"))
```
If Age increases from 25 to 26, earnings are  expected to increase by 2.55%. If Age increases from 33 to 34, earnings are also expected to increase by 2.55%.

### c.
```{r}
# add column of log(age) in dataset
cps12 <- cps12 %>% mutate(logage = log(age))
# Run a regression of the logarithm of average hourly earnings, ln(AHE), on ln(Age), Female, and Bachelor.
reg_ahe_3 <- lm(logearning~logage+female+bachelor, data = cps12)
coeftest(reg_ahe_3, vcov = vcovHC(reg_ahe_3, type = "HC1"))
```
If Age increases from 25 to 26(4%), earnings are expected to increase by 4 x 0.753% = 3.012%. If Age increases from 33 to 34(3.03%), earnings are also expected to increase by 3.03 x 0.753% = 2.282%.


### d.
```{r}
# add column of age^2 in dataset
cps12 <- cps12 %>% mutate(squareage = age^2)
# Run a regression of the logarithm of average hourly earnings, ln(AHE), on Age, Age^2, Female, and Bachelor. 
reg_ahe_4 <- lm(logearning~age+squareage+female+bachelor, data = cps12)
coeftest(reg_ahe_4, vcov = vcovHC(reg_ahe_4, type = "HC1"))
# Caculate change in AHE if age increases from 25 to 26
1*0.104 + (26^2-25^2)*(-0.001328)
# Caculate change in AHE if age increases from 33 to 34
1*0.104 + (34^2-33^2)*(-0.001328)
```
If Age increases from 25 to 26, earnings are expected to increase by 3.63%. If Age increases from 33 to 34(3.03%), earnings are also expected to increase by 1.50%.

### e.
```{r}
# get adjusted R-square for regression in b
summary.lm(reg_ahe_2)["adj.r.squared"][[1]]
# get adjusted R-square for regression in c
summary.lm(reg_ahe_3)["adj.r.squared"][[1]]
```
I prefer regression (c) over regression (b) because the adjusted r-squared for regression (c)(0.1962391) is greater than the adjusted r-squared for regression (b)(0.19606). Therefore, regression (c) has a better fit.

### f.
```{r}
# get adjusted R-square for regression in b
summary.lm(reg_ahe_2)["adj.r.squared"][[1]]
# get adjusted R-square for regression in d
summary.lm(reg_ahe_4)["adj.r.squared"][[1]]
```
I prefer regression (d) over regression (b) because the adjusted r-squared for regression (b)(0.1962726) is greater than the adjusted r-squared for regression (b)(0.19606). Therefore, regression (d) has a better fit.

### g.
```{r}
# get adjusted R-square for regression in c
summary.lm(reg_ahe_3)["adj.r.squared"][[1]]
# get adjusted R-square for regression in d
summary.lm(reg_ahe_4)["adj.r.squared"][[1]]
```
I prefer regression (d) over regression (c) because the adjusted r-squared for regression (d)(0.1962726) is greater than the adjusted r-squared for regression (c)(0.1962726). Therefore, regression (d) has a better fit.


### h.
```{r}
# plot for regression in (b)
library(ggplot2)
plot_b<-function(x)1.94142 + 0.02552*x
ggplot(reg_ahe_2, aes(x = age, y = logearning))+
  stat_function(fun = plot_b)
# plot for regression in (c)
plot_c<-function(x)0.1495 +0.7529*x
ggplot(reg_ahe_3, aes(x = logage, y = logearning))+
  stat_function(fun = plot_c)
# plot for regression in (d)
plot_d<-function(x) 0.791882 + 0.104045*x -0.001328*x^2 
ggplot(reg_ahe_4, aes(x = age, y = logearning))+ 
  stat_function(fun = plot_d)

```

Plot b and plot c have straight lines and are quite similar as there is linear relatinship between log of AHE(logearning) and age or log of age. The slopes for plot b and plot c are constant. However, plot d is a curve line, which indicates the non-linear relationship between log of AHE and age. This is generated due to the addition of regressor squareage(the square of age). The slope for plot d decreases as the value of age increases.

My answer would not change if I plot the regression function for females with college degrees. Because there are no interaction terms in all the three regressions, the effect of age on In(ahe) will not depend on the binary variable female.

### i.
```{r}
# add column female*bachelor to dataset
cps12 <- cps12 %>% mutate(female_bachelor = female*bachelor)
reg_ahe_5 <- lm(logearning~age+squareage+female+bachelor+female_bachelor, data = cps12)
coeftest(reg_ahe_5, vcov = vcovHC(reg_ahe_5, type = "HC1"))
# In(ahe) for Alexis
ahe_Alexis = 0.80374070 + 30*0.10432241-0.00133158*30^2-0.24237318 *1+0.40044634*1+0.08985710*1*1
ahe_Alexis
# In(ahe) for Jane
ahe_Jane =  0.80374070 + 30*0.10432241-0.00133158*30^2-0.24237318 *1+0.40044634*0+0.08985710*1*0
ahe_Jane
# difference between In(ahe) for Alexis and In(ahe) for Jane
difference_1 = ahe_Alexis-ahe_Jane
difference_1
# In(ahe) for Bob
ahe_Bob = 0.80374070 + 30*0.10432241-0.00133158*30^2-0.08985710*0+0.40044634*1+0.08985710*0*1
ahe_Bob
# In(ahe) for Jim
ahe_Jim = 0.80374070 + 30*0.10432241-0.00133158*30^2-0.08985710*0+0.40044634*0+0.08985710*0*0
ahe_Jim
# difference between In(ahe) for Bob and In(ahe) for Jim
difference_2 = ahe_Bob-ahe_Jim
difference_2
```
The coefficient on internaction term(0.0899) measures the difference of effect of having a bachelor degree on In(ahe) for female vs. male.

The regression predicts that the In(ahe) for Alexis is 2.983. The regression predicts that the In(ahe) for Jane is 2.492. The predicted difference between Alexis’s and Jane’s earnings is 0.490.
The regression predicts that the In(ahe) for Bob is 3.135. The regression predicts that the In(ahe) for Jim is 2.734. The predicted difference between Alexis’s and Jane’s earnings is 0.400.



### j.
```{r}
# add columns female*age, femalge*(age^2) to dataset
cps12 <- cps12 %>% mutate(female_age = female*age, female_agesquare = female*age*age)
reg_ahe_6 <- lm(logearning~age+squareage+female+bachelor+female_bachelor+female_age+female_agesquare, data = cps12)
coeftest(reg_ahe_6, vcov = vcovHC(reg_ahe_6, type = "HC1"))
```
Yes, the effect of Age on earnings is different for men than for women. Through adding two more interaction terms into the regression(female_age, female_agesquare), we can tell that the t-stat for these two variables are statistically significant at 5% level.

### k.
```{r}
# add columns bachelor*age, bachelor*(age^2) to dataset
cps12 <- cps12 %>% mutate(bachelor_age = bachelor*age, bachelor_agesquare = bachelor*age*age)
reg_ahe_7 <- lm(logearning~age+squareage+female+bachelor+female_bachelor+bachelor_age+bachelor_agesquare, data = cps12)
coeftest(reg_ahe_7, vcov = vcovHC(reg_ahe_7, type = "HC1"))
```
No, the effect of Age on earnings is not different for high school graduates than for college graduates. Through adding two more interaction terms into the regression(bachelor_age, bachelor_agesquare), we can tell that the t-stat for these two variables are not statistically significant at any level.

### l.
Based on the above regression, age has a positive effect on earnings. The degree of effect will also depend on the worker's gender and education level. The positive effect will get larger if the work is a male or/and have a bachelor degree.

